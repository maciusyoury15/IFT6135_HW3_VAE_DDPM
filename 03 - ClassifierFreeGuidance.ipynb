{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "\n",
    "In this assignment, you will implement a [Classifier Free Guidance model](https://arxiv.org/pdf/2207.12598) class on MNIST dataset using PyTorch according to the guidence. The goal is to minimize the loss function and train the model to generate MNIST images with conditions on label.\n",
    "\n",
    "The `Train` and `UNet` classes are already implemented for you. You need to implement the `CFGDiffusion` class (see details below). The images generated by the model will be automatically shown according to the `Trainer` class implementation. Make sure the generated images are shown in the output, it will be graded.\n",
    "\n",
    "Grade:\n",
    "- Explain why is the model called Classifier Free  and why Guidance (5 points).\n",
    "- According to the paper, what would be an alternative of classifier free ? Explain how would the loss change in this alternative compared to the original DDPM loss ? (5 points)\n",
    "- Implement CFGDiffusion class (20 points)\n",
    "- Complete the Trainer.sample() method (10 points)\n",
    "- Write a report to describe the sampled images generated by each epochs (5 points).\n",
    "\n",
    "**Please note that the function to generate the images is already provided.**\n",
    "\n",
    "---\n",
    "Please DO NOT change the code provided, only add your own code where indicated. It is recommended that you **use CPU session to debug** when GPU is not necessary since Colab only gives 12 hrs of free GPU access at a time. If you use up the GPU resource, you may consider using Kaggle GPU resource. Thank you and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-determined config and given functions (no need to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following files to your directory:\n",
    "- args.py\n",
    "- unet.py\n",
    "- datasets.py\n",
    "- utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "from cfg_utils.args import *\n",
    "from cfg_utils.dataset import *\n",
    "from cfg_utils.unet import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda backend\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {args.device} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Classifier Free Guidance Model \n",
    "\n",
    "To that end, refer to the training and sampling algorithms from the paper as well as the different equations. Less description is included here so that you're forced to learn how to refer to a paper. Still, note that guidences are also here to help you with what to fill in each function\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T15:21:06.221374Z",
     "start_time": "2025-05-02T15:21:06.203921Z"
    }
   },
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from typing import Tuple, Optional\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "\n",
    "from cfg_utils.args import *\n",
    "from q1_train_vae import loss_function\n",
    "\n",
    "\n",
    "class CFGDiffusion():\n",
    "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.eps_model = eps_model\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        self.lambda_min = -20\n",
    "        self.lambda_max = 20\n",
    "\n",
    "    ### UTILS\n",
    "    def get_exp_ratio(self, l: torch.Tensor, l_prim: torch.Tensor):\n",
    "        return torch.exp(l-l_prim)\n",
    "\n",
    "    def get_lambda(self, t: torch.Tensor):\n",
    "        u = t / self.n_steps\n",
    "\n",
    "        device = t.device\n",
    "        lambda_min = torch.tensor(self.lambda_min, device=device)\n",
    "        lambda_max = torch.tensor(self.lambda_max, device=device)\n",
    "\n",
    "        b = torch.atan(torch.exp(-lambda_max / 2))\n",
    "        a = torch.atan(torch.exp(-lambda_min / 2)) - b\n",
    "\n",
    "        lambda_t = -2 * torch.log(torch.tan(a * u + b))\n",
    "        return lambda_t.view(-1, 1, 1, 1)\n",
    "\n",
    "    def alpha_lambda(self, lambda_t: torch.Tensor):\n",
    "        alpha_squared = 1 / (1 + torch.exp(-lambda_t))\n",
    "        return torch.sqrt(alpha_squared)\n",
    "\n",
    "    def sigma_lambda(self, lambda_t: torch.Tensor):\n",
    "        alpha = self.alpha_lambda(lambda_t)\n",
    "        sigma_squared = 1 - alpha ** 2\n",
    "        return torch.sqrt(sigma_squared)\n",
    "\n",
    "    ## Forward sampling\n",
    "    def q_sample(self, x: torch.Tensor, lambda_t: torch.Tensor, noise: torch.Tensor):\n",
    "        alpha = self.alpha_lambda(lambda_t)\n",
    "        sigma = self.sigma_lambda(lambda_t)\n",
    "        return alpha * x + sigma * noise\n",
    "\n",
    "    def sigma_q(self, lambda_t: torch.Tensor, lambda_t_prim: torch.Tensor):\n",
    "        sigma_lambda_sq = self.sigma_lambda(lambda_t) ** 2\n",
    "        ratio = 1 - torch.exp(lambda_t - lambda_t_prim)\n",
    "        var = ratio * sigma_lambda_sq\n",
    "        return torch.sqrt(torch.clamp(var, min=1e-10))\n",
    "\n",
    "    def sigma_q_x(self, lambda_t: torch.Tensor, lambda_t_prim: torch.Tensor):\n",
    "        sigma_lambda_sq = self.sigma_lambda(lambda_t_prim) ** 2\n",
    "        ratio = 1 - torch.exp(lambda_t - lambda_t_prim)\n",
    "        var = ratio * sigma_lambda_sq\n",
    "        return torch.sqrt(torch.clamp(var, min=1e-10))\n",
    "\n",
    "    ### REVERSE SAMPLING\n",
    "    def mu_p_theta(self, z_lambda_t: torch.Tensor, x: torch.Tensor, lambda_t: torch.Tensor, lambda_t_prim: torch.Tensor):\n",
    "        exp_ratio = torch.exp(lambda_t - lambda_t_prim)\n",
    "        alpha_t = self.alpha_lambda(lambda_t)\n",
    "        alpha_t_prim = self.alpha_lambda(lambda_t_prim)\n",
    "\n",
    "        term1 = exp_ratio * (alpha_t_prim / alpha_t) * z_lambda_t\n",
    "        term2 = (1 - exp_ratio) * alpha_t_prim * x\n",
    "        return term1 + term2\n",
    "\n",
    "    def var_p_theta(self, lambda_t: torch.Tensor, lambda_t_prim: torch.Tensor, v: float=0.3):\n",
    "        sigma_lambda_sq = self.sigma_lambda(lambda_t) ** 2\n",
    "        sigma_lambda_prim_sq = self.sigma_lambda(lambda_t_prim) ** 2\n",
    "        sigma_ratio = 1 - torch.exp(lambda_t - lambda_t_prim)\n",
    "\n",
    "        base_var = sigma_ratio * sigma_lambda_sq\n",
    "        base_var_prim = sigma_ratio * sigma_lambda_prim_sq\n",
    "\n",
    "        # According to Eq (4): interpolate variance\n",
    "        return torch.clamp((base_var_prim ** (1 - v)) * (base_var ** v), min=1e-10)\n",
    "\n",
    "    def p_sample(self, z_lambda_t: torch.Tensor, lambda_t : torch.Tensor, lambda_t_prim: torch.Tensor,  x_t: torch.Tensor, set_seed=False):\n",
    "        if set_seed:\n",
    "            torch.manual_seed(42)\n",
    "\n",
    "        mu = self.mu_p_theta(z_lambda_t, x_t, lambda_t, lambda_t_prim)\n",
    "        var = self.var_p_theta(lambda_t, lambda_t_prim)\n",
    "\n",
    "        noise = torch.randn_like(z_lambda_t)\n",
    "        sample = mu + torch.sqrt(var) * noise\n",
    "\n",
    "        return sample\n",
    "\n",
    "    ### LOSS\n",
    "    def loss(self, x0: torch.Tensor, labels: torch.Tensor, noise: Optional[torch.Tensor] = None, set_seed=False):\n",
    "        if set_seed:\n",
    "            torch.manual_seed(42)\n",
    "\n",
    "        batch_size = x0.shape[0]\n",
    "        dim = tuple(range(1, x0.ndim))\n",
    "\n",
    "        # Step 1: Sample t ~ Uniform({0, ..., T-1})\n",
    "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n",
    "\n",
    "        # Step 2: Get λ(t)\n",
    "        lambda_t = self.get_lambda(t)  # (batch_size, 1, 1, 1)\n",
    "\n",
    "        # Step 3: Sample noise ε ~ N(0, I)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "\n",
    "        # Step 4: Compute z_λ = α(λ) * x0 + σ(λ) * ε\n",
    "        z_lambda = self.q_sample(x0, lambda_t, noise)\n",
    "\n",
    "        # Step 5: Predict noise using ε_θ(z_λ, labels)\n",
    "        eps_pred = self.eps_model(z_lambda, labels)\n",
    "\n",
    "        loss = (eps_pred - noise) ** 2  # (batch_size, C, H, W)\n",
    "        loss = loss.sum(dim=dim).mean()  # sum over pixels, mean over batch\n",
    "\n",
    "        return loss"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish implementation of the Trainer.sample() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from q3_trainer_cfg import *\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args, eps_model, diffusion_model):\n",
    "\n",
    "        self.eps_model = eps_model.to(args.device)\n",
    "\n",
    "        self.diffusion = diffusion_model\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.eps_model.parameters(), lr=args.learning_rate\n",
    "        )\n",
    "        self.args = args\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.ema = EMA(0.995)\n",
    "        self.ema_model = copy.deepcopy(self.eps_model).eval().requires_grad_(False)\n",
    "\n",
    "\n",
    "    def train_epoch(self, dataloader, scaler):\n",
    "        current_lr = round(self.optimizer.param_groups[0]['lr'], 8)\n",
    "        i = 0\n",
    "        running_loss = 0.\n",
    "        with tqdm(range(len(dataloader)), desc=f'Epoch : - lr: - Loss :') as progress:\n",
    "            for x0, labels in dataloader:\n",
    "                i += 1\n",
    "                # Move data to device\n",
    "                x0 = x0.to(self.args.device)\n",
    "                # Use guidance\n",
    "                labels = labels.to(self.args.device)\n",
    "                if np.random.random() < 0.1:\n",
    "                    labels = None\n",
    "\n",
    "                # Calculate the loss\n",
    "                with autocast(device_type=self.args.device, enabled=self.args.fp16_precision):\n",
    "                    loss = self.diffusion.loss(x0, labels)\n",
    "\n",
    "                # Zero gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "                # loss.backward()\n",
    "                # self.optimizer.step()\n",
    "                self.ema.step_ema(self.ema_model, self.eps_model)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                self.loss_per_iter.append(running_loss / i)\n",
    "                progress.update()\n",
    "                progress.set_description(f'Epoch: {self.current_epoch}/{self.args.epochs} - lr: {current_lr} - Loss: {round(running_loss / i, 2)}')\n",
    "            progress.set_description(f'Epoch: {self.current_epoch}/{self.args.epochs} - lr: {current_lr} - Loss: {round(running_loss / len(dataloader), 2)}')\n",
    "\n",
    "            # Step the scheduler after each epoch\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def train(self, dataloader):\n",
    "            scaler = GradScaler(device=self.args.device, enabled=self.args.fp16_precision)\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.5)\n",
    "            start_epoch = self.current_epoch\n",
    "            self.loss_per_iter = []\n",
    "            for current_epoch in range(start_epoch, self.args.epochs):\n",
    "                self.current_epoch = current_epoch\n",
    "                self.train_epoch(dataloader, scaler)\n",
    "                if current_epoch % self.args.show_every_n_epochs == 0:\n",
    "                    self.sample(cfg_scale=self.args.cfg_scale)\n",
    "\n",
    "                if (current_epoch + 1) % self.args.save_every_n_epochs == 0:\n",
    "                    self.save_model()\n",
    "\n",
    "    def sample(self, labels=None, cfg_scale=3., n_steps=None, set_seed=False):\n",
    "        if set_seed:\n",
    "            torch.manual_seed(42)\n",
    "        if n_steps is None:\n",
    "            n_steps = self.args.n_steps\n",
    "\n",
    "        self.eps_model.eval()\n",
    "        with torch.no_grad():\n",
    "            z_t = torch.randn(\n",
    "                    [\n",
    "                        self.args.n_samples,\n",
    "                        self.args.image_channels,\n",
    "                        self.args.image_size,\n",
    "                        self.args.image_size,\n",
    "                    ],\n",
    "                    device=self.args.device,\n",
    "                )\n",
    "\n",
    "            if labels == None:\n",
    "                labels = torch.randint(0, 9, (self.args.n_samples,), device=self.args.device)\n",
    "\n",
    "            if self.args.nb_save is not None:\n",
    "                saving_steps = [n_steps - 1]\n",
    "\n",
    "            # Remove noise for $T$ steps\n",
    "            for t_ in tqdm(range(n_steps)):\n",
    "                t_val = n_steps - 1 - t_\n",
    "                t = torch.full((self.args.n_samples,), t_val, device=z_t.device, dtype=torch.long)\n",
    "                # t_prim = torch.full((self.args.n_samples,), t_val+1, device=z_t.device, dtype=torch.long)\n",
    "\n",
    "                lambda_t = self.diffusion.get_lambda(t)\n",
    "                lambda_t_prim = self.diffusion.get_lambda(t - 1)\n",
    "\n",
    "                eps_cond = self.eps_model(z_t, labels)\n",
    "                eps_uncond = self.eps_model(z_t, None)\n",
    "                eps_theta = (1 + cfg_scale) * eps_cond - cfg_scale * eps_uncond\n",
    "\n",
    "                alpha_lambda = self.diffusion.alpha_lambda(lambda_t)\n",
    "                sigma_lambda = self.diffusion.sigma_lambda(lambda_t)\n",
    "                x_t = (z_t - sigma_lambda * eps_theta) / alpha_lambda\n",
    "\n",
    "                z_t = self.diffusion.p_sample(z_t, lambda_t, lambda_t_prim, x_t)\n",
    "\n",
    "                if self.args.nb_save is not None and t_ in saving_steps:\n",
    "                    print(f\"Showing/saving samples from epoch {self.current_epoch} with labels: {labels.tolist()}\")\n",
    "                    self.show_save(\n",
    "                        img_tensor=x_t,\n",
    "                        labels=labels,\n",
    "                        show=True,\n",
    "                        save=True,\n",
    "                        file_name=f\"CFG_epoch_{self.current_epoch}_sample_{t_}.png\"\n",
    "                    )\n",
    "\n",
    "            self.eps_model.train()\n",
    "        return x_t\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save({\n",
    "                'epoch': self.current_epoch,\n",
    "                'model_state_dict': self.eps_model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                }, self.args.MODEL_PATH)\n",
    "\n",
    "    def show_save(self, img_tensor, labels=None, show=True, save=True, file_name=\"sample.png\"):\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(10, 10))  # Create a 4x4 grid of subplots\n",
    "        assert img_tensor.shape[0] >= 9, \"Number of images should be at least 9\"\n",
    "        img_tensor = img_tensor[:9]\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            # Remove the channel dimension and convert to numpy\n",
    "            img = img_tensor[i].squeeze().cpu().numpy()\n",
    "            label = labels[i].item()\n",
    "            ax.imshow(img, cmap=\"gray\")  # Display the image in grayscale\n",
    "            ax.set_title(f'Digit:{label}')\n",
    "            ax.axis(\"off\")  # Hide the axis\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            os.makedirs(\"images\", exist_ok=True)\n",
    "            plt.savefig(os.path.join(\"images\", file_name))\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    MNISTDataset(),\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "eps_model = UNet_conditional(c_in=1, c_out=1, num_classes=10)\n",
    "\n",
    "diffusion_model = CFGDiffusion(\n",
    "            eps_model=eps_model,\n",
    "            n_steps=args.n_steps,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(args, eps_model, diffusion_model)\n",
    "\n",
    "trainer.train(dataloader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
